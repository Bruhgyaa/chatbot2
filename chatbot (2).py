# -*- coding: utf-8 -*-
"""chatbot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dzB_PWcLuDxe_FMfvocALJ6ws2NWk8y3
"""

!pip install langchain

from langchain import PromptTemplate

from langchain.vectorstores import FAISS
from langchain.llms import GooglePalm
from langchain.document_loaders.csv_loader import CSVLoader
from langchain.embeddings import HuggingFaceInstructEmbeddings
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
import os

!pip install python-dotenv==1.0.0
!pip install streamlit==1.22.0
!pip install tiktoken==0.4.0
!pip install faiss-cpu==1.7.4
!pip install protobuf~=3.19.0

from langchain_community.document_loaders import CSVLoader

# Specify the appropriate encoding
encoding = 'latin1'  # or 'utf-16' or any other encoding you suspect might work

# Initialize the CSVLoader with the specified encoding
loader = CSVLoader(file_path='/content/codebasics_faqs.csv', source_column='prompt', encoding=encoding)

# Load the data
data = loader.load()

data

!pip install sentence_transformers
!pip install InstructorEmbedding

!pip uninstall sentence-transformers
!pip install sentence-transformers==2.2.2

embeddings = HuggingFaceInstructEmbeddings(
    query_instruction="Represent the query for retrieval: "
)

e = embeddings.embed_query("what is your refund policy")

len(e)

e[:5]

from langchain.vectorstores import FAISS

# Create a FAISS instance for vector database from 'data'
instructor_embeddings = HuggingFaceInstructEmbeddings()
vectordb = FAISS.from_documents(documents=data,
                                 embedding=instructor_embeddings)

retriever = vectordb.as_retriever()
rdocs = retriever.get_relevant_documents("how about job placement support?")
rdocs

api_key = "AIzaSyAUnzYslfB7BTIaEJH9CAw943eseQJ2Lsc"
llm = GooglePalm(google_api_key=api_key, temperature=0.7)

from langchain.prompts import PromptTemplate

prompt_template = """Given the following context and a question, generate an answer based on this context only.
In the answer try to provide as much text as possible from "response" section in the source document context without making much changes.
If the answer is not found in the context, kindly state "I don't know." Don't try to make up an answer.

CONTEXT: {context}

QUESTION: {question}"""


PROMPT = PromptTemplate(
    template=prompt_template, input_variables=["context", "question"]
)
chain_type_kwargs = {"prompt": PROMPT}
from langchain.chains import RetrievalQA
chain = RetrievalQA.from_chain_type(llm=llm,
                            chain_type="stuff",
                            retriever=retriever,
                            input_key="query",
                            return_source_documents=True,
                                    chain_type_kwargs={"prompt":PROMPT})

chain('Do you provide job assistance and also do you provide job gurantee?')

chain("Do you guys provide internship and also do you offer EMI payments?")

!pip install streamlit

